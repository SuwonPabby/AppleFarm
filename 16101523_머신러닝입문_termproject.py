# -*- coding: utf-8 -*-
"""16101523_머신러닝입문_TermProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5IZj7wBgRwO63_qCypoEt7A4uV2k5yA

# **Real News와 Fake News 분류 모델**    
 16101523 이의진 
 ----------


## *개요*

 > 사회가 돌아가는 소식을 우리는 보통 종이 신문보다 인터넷 기사가 익숙한 시대에 살고 있다. 간단한 검색만으로 도 우리가 원하는 소식을 접할 수 있어 매우 빠르고 편리하다. 그러나 그와 동시에 과거에 비해 가짜 뉴스의 빈도도 늘어 사회적으로 큰 문제가 되고있다. 특히 잘못 편향된 뉴스로 인해 수많은 악플이 생기고 이로 인해 많은 사람들이 불편을 호소하고 있다.
 
 > 그래서 이번 개인 프로젝트에서는 자연어 처리 머신러닝 알고리즘을 이용하여 뉴스가 진짜 뉴스인지 가짜 뉴스인 지 판별하는 Task를 수행하고, 더불어 가짜 뉴스와 진짜 뉴스에 드러나는 특징 데이터들에 대해 데이터 시각화를 진행하여 가짜 뉴스와 진짜 뉴스 상에 보이는 차이점을 분석해보는 활동을 진행해보고자 한다.

# Load Dataset
* Dataset : Kaggle - Fake and Real News Dataset
* Structure of DataSet
  * This dataset consists of 2 CSV files
     * Fake.csv : The dataset whose datas are about fake news 
     * Real.csv : The dataset whose datas are about real news
  * Each CSV file contains 4 column : title, text, subject, date
     * title : title of the news
     * text : contents of the news
     * subject : subject of the news
     * date : information about when the news are written
  * Data size
     * Real news dataset has 21417 datas
     * Fake news dataset has 23481 datas

## Load dataset which saved in my google drive
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

real = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/머신러닝입문/dataset/Real.csv')
real

fake = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/머신러닝입문/dataset/Fake.csv')
fake

"""## Verify existence of NaN Data 

if it exists, remove it 
"""

real.isna().sum()  # No NaN Data in real.csv

fake.isna().sum() # No NaN Data in fake.csv

"""## Summary of DataFrame : real.CSV"""

real.shape

real.describe()

tab = real.groupby('subject').size()
tab

"""Subjects of Real News"""

sns.set_palette("bwr", 2)
sns.countplot(real['subject'])
plt.title("Subjects of Real News")
plt.xticks(rotation = 60)

"""When the real news were written?"""

real['month'] =  real['date'].str.split(' ', expand = True).iloc[:,0]
real.head()

table = real.month.value_counts()
sns.set_palette("icefire", 12)
sns.barplot(table.index, table)
plt.title("Written Date of Real News")
plt.xticks(rotation = 60)

"""## Summary of DataFrame : fake.CSV"""

fake.shape

fake.describe

tab = fake.groupby('subject').size()
tab

sns.set_palette("RdPu_r", 6)
sns.countplot(fake['subject'])
plt.title("Subjects of Fake News")
plt.xticks(rotation = 60)

"""When the fake news were written?"""

fake['month'] =  fake['date'].str.split(' ', expand = True).iloc[:,0]
fake.head()

table = fake.month.value_counts().head(12)
sns.set_palette("inferno", 12)
sns.barplot(table.index, table)
plt.title("Written Date of Fake News")
plt.xticks(rotation = 60)

"""## Analysis

우선 데이터 셋에서 Subject와 Date에 대한 자료 분포에서 유의미한 점이 있는지를 관찰해보았다. 

Subject term의 경우 애초에 Real News는 2개 분류, Fake News 는 6개 분류라 Shape이 일치하지 않고,

공통 2개 분류에 해당하는 politics와 News 역시 Fake와 Real 모두 비슷한 분포를 가지기에 유의미하지 않기에 굳이 모델에 넣을 필요가 없다.

Date term의 경우 이 자료 분석을 통해 Real News는 연말에, Fake News는 5월 경에 많았다는 정보를 알 수 있다.

그러나 우리는 어차피 새로운 Data에 대한 Classifing을 진행해야하고 해당 뉴스들은 모두 2017년 기준의 Date 정보를 가지고 있으니 모델에 Date 정보를 넣을 필요는 없다.

# PreProcessing Data
"""

def make_input_data(vec, data, label, column):
  x = vec.fit_transform(data[column])
  y = [label] * len(x)
  return x, y

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer


token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(ngram_range=(1,1), lowercase = True, stop_words = 'english', tokenizer = token.tokenize)
tfidf = TfidfVectorizer()